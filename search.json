[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Proyek Sains Data",
    "section": "",
    "text": "1 Proyek Sains Data\nProyek sains data adalah suatu usaha yang dilakukan untuk mengumpulkan, menganalisis, dan menginterpretasi data dengan tujuan untuk mendapatkan wawasan, mendukung pengambilan keputusan, atau memecahkan masalah tertentu. Proyek ini melibatkan penerapan metode statistik, matematika, dan teknik komputasi untuk mengeksplorasi dan memahami pola atau tren dalam data.\nLangkah-langkah umum dalam proyek sains data melibatkan:\n\nPemahaman Masalah: Menentukan tujuan proyek dan pemahaman mendalam tentang masalah yang ingin dipecahkan.\nPengumpulan Data: Mengumpulkan data yang relevan dan diperlukan untuk proyek.\nPembersihan Data: Membersihkan dan mempersiapkan data untuk analisis dengan mengatasi masalah seperti data yang hilang atau outlier.\nEksplorasi Data: Mengeksplorasi data untuk mengidentifikasi pola, tren, atau informasi yang dapat diambil.\nPemodelan: Menerapkan teknik statistik atau pembelajaran mesin untuk membangun model yang dapat meramalkan atau menjelaskan fenomena yang diamati.\nEvaluasi Model: Menilai kinerja model dan memastikan bahwa solusi yang diusulkan sesuai dengan tujuan awal proyek.\nImplementasi: Menerapkan solusi atau rekomendasi berdasarkan hasil analisis data.\nKomunikasi Hasil: Mengkomunikasikan temuan dan rekomendasi kepada pemangku kepentingan melalui laporan atau presentasi.\n\nProyek sains data dapat diterapkan dalam berbagai konteks, termasuk bisnis, ilmu pengetahuan, kesehatan, dan banyak lagi. Penting untuk mencatat bahwa sains data bukan hanya tentang alat dan teknik analisis, tetapi juga melibatkan pemahaman domain yang baik untuk menghasilkan hasil yang bermakna.\nAuthor: Nama: Whinta Virginia Putri NIM: 210411100047 Proyek Sains Data A"
  },
  {
    "objectID": "Prediksi_Audio_Emosi.html#deskripsi-dataset",
    "href": "Prediksi_Audio_Emosi.html#deskripsi-dataset",
    "title": "2  Analisis dan Prediksi pada Data Audio Emosi",
    "section": "2.1 Deskripsi Dataset :",
    "text": "2.1 Deskripsi Dataset :\nAda sekitar 200 kata target yang diucapkan dalam frase pembawa ‘say the word_’ oleh dua aktris (berusia 26 dan 64 tahun) dan rekaman dibuat dari set tersebut menggambarkan tujuh emosi (marah, jijik, takut, bahagia, terkejut menyenangkan, sedih, dan netral). Terdapat total 2800 titik data (file audio).\nKumpulan data ini diatur sedemikian rupa sehingga setiap dari dua aktris perempuan dan emosi mereka terdapat dalam folder tersendiri. Dan di dalamnya, semua file audio 200 kata target dapat ditemukan. Format file audio ini adalah format WAV.\nBerikut link kaggle dataset: https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess?resource=download"
  },
  {
    "objectID": "Prediksi_Audio_Emosi.html#pre-processing",
    "href": "Prediksi_Audio_Emosi.html#pre-processing",
    "title": "2  Analisis dan Prediksi pada Data Audio Emosi",
    "section": "2.2 Pre-processing",
    "text": "2.2 Pre-processing\nDibuat 11 fitur dari Dataset audio kaggle yaitu Label, Mean, Median, Std_Deviation, Zero_Crossing_Rate, Energy, Spectral_Centroid, Spectral_Bandwidth, Spectral_Rolloff, Chroma, dan MFCCs dari 2191 data, Kemudian di simpan pada format csv. Berikut adalah hasil Audio_features:\n\ndf = pd.read_csv('https://raw.githubusercontent.com/whintaaa/datapsd/main/audio_fitur.csv')\ndf\n\n\n  \n    \n\n\n\n\n\n\nLabel\nAudio_Name\nMean\nMedian\nStd_Deviation\nZero_Crossing_Rate\nEnergy\nSpectral_Centroid\nSpectral_Bandwidth\nSpectral_Rolloff\nChroma\nMFCCs\n\n\n\n\n0\ndisgust\nYAF_chief_disgust\n-0.000022\n-0.000181\n0.022747\n0.232713\n0.017588\n3447.960816\n2174.962231\n5621.252553\n0.391336\n-30.599363\n\n\n1\ndisgust\nYAF_chain_disgust\n-0.000018\n-0.000350\n0.020867\n0.207239\n0.015964\n3139.689603\n2052.783771\n4998.484497\n0.342745\n-30.325106\n\n\n2\ndisgust\nYAF_base_disgust\n-0.000021\n-0.000203\n0.022053\n0.246661\n0.017039\n3609.513207\n2183.848242\n5606.822791\n0.348650\n-30.605677\n\n\n3\ndisgust\nYAF_ditch_disgust\n-0.000024\n-0.000103\n0.020262\n0.196533\n0.015652\n3202.293486\n2061.560496\n5041.901634\n0.365224\n-30.454414\n\n\n4\ndisgust\nYAF_germ_disgust\n-0.000018\n-0.000353\n0.025729\n0.192057\n0.020146\n2898.874228\n1902.174869\n4483.722888\n0.321074\n-30.068130\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2185\nSad\nOAF_wheat_sad\n0.000033\n0.000606\n0.010872\n0.086951\n0.008632\n1816.902526\n2151.488369\n3230.488327\n0.252862\n-32.830270\n\n\n2186\nSad\nOAF_puff_sad\n-0.000010\n0.000292\n0.013971\n0.078135\n0.011377\n1743.116982\n2153.268985\n3179.314108\n0.375577\n-32.812740\n\n\n2187\nSad\nOAF_raise_sad\n-0.000001\n0.000284\n0.013523\n0.086271\n0.011093\n1922.328544\n2198.764520\n3393.786621\n0.297995\n-33.027540\n\n\n2188\nSad\nOAF_sail_sad\n0.000032\n0.000330\n0.010211\n0.118340\n0.008695\n2245.346145\n2210.531313\n3846.132299\n0.257481\n-32.356060\n\n\n2189\nSad\nOAF_rat_sad\n-0.000013\n0.000459\n0.010945\n0.079590\n0.009279\n1725.299469\n2012.117700\n2973.918181\n0.268717\n-33.213444\n\n\n\n\n\n2190 rows × 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Memisahkan fitur (X) dan label (y)\nX = df.drop(['Label','Audio_Name'], axis=1)\ny = df['Label']\n# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.2)\n\n\n2.2.1 Normalisasi Scaler\nNormalisasi scaler dalam konteks kode yang Anda berikan tampaknya menggunakan StandardScaler dari pustaka scikit-learn untuk melakukan normalisasi pada data. Normalisasi adalah proses mengubah nilai-nilai dalam dataset ke skala umum, biasanya dengan mengurangkan rata-rata dan membagi hasilnya dengan deviasi standar.\nMari kita bahas langkah-langkah dalam kode tersebut:\n\nDefine and Fit Scaler:\nscaler = StandardScaler()\nscaler.fit(X_train)\nPada langkah ini, Anda membuat objek StandardScaler dari scikit-learn dan kemudian menggunakan metode fit untuk menghitung rata-rata dan deviasi standar dari dataset pelatihan (X_train).\nSave Scaler Using Pickle:\nscaler_file_path = r'scaler.pkl'\nwith open(scaler_file_path, 'wb') as scaler_file:\n    pickle.dump(scaler, scaler_file)\nPada langkah ini, Anda menyimpan objek scaler ke dalam file menggunakan modul pickle. Ini memungkinkan Anda untuk menggunakan kembali objek scaler tanpa harus melatih ulang model setiap kali Anda menjalankan program.\nTransform Training Data Using Scaler:\nX_train_scaled = scaler.transform(X_train)\nDi sini, Anda menggunakan scaler yang telah dilatih untuk melakukan normalisasi pada dataset pelatihan (X_train) dan menyimpan hasilnya dalam X_train_scaled.\nLoad Scaler Using Pickle:\nwith open(r'scaler.pkl', 'rb') as normalisasi:\n    loadscal = pickle.load(normalisasi)\nLangkah terakhir adalah memuat kembali objek scaler dari file yang telah disimpan sebelumnya menggunakan modul pickle. Objek scaler yang dimuat disimpan dalam variabel loadscal.\n\nDengan cara ini, Anda dapat menggunakan objek loadscal untuk melakukan transformasi normalisasi pada dataset lain dengan menggunakan parameter normalisasi yang sama seperti yang telah dihitung pada dataset pelatihan.\n\n# Define and fit the scaler on the training dataset\nscaler = StandardScaler()\nscaler.fit(X_train)\n# Save the scaler using pickle\nscaler_file_path = r'scaler.pkl'\nwith open(scaler_file_path, 'wb') as scaler_file:\n    pickle.dump(scaler, scaler_file)\n\nX_train_scaled = scaler.transform(X_train)\nwith open(r'scaler.pkl', 'rb') as normalisasi:\n    loadscal = pickle.load(normalisasi)\n\n\nX_test_scaled = loadscal.transform(X_test)\n\n\nX_test_scaled\n\narray([[-0.36438935, -0.01864303,  0.62951586, ...,  1.700305  ,\n         1.54756458,  0.50833751],\n       [-0.4145327 , -0.16604988,  0.48640237, ...,  0.96098207,\n        -0.94090678,  0.60164653],\n       [-0.54970173, -0.37708373,  0.00366902, ...,  0.23587569,\n        -0.06940957,  0.06954893],\n       ...,\n       [-0.47993707, -0.22850376, -0.05819755, ...,  0.90422257,\n         0.30463648, -0.36001778],\n       [ 0.80198861, -0.0505326 , -1.00057672, ...,  0.02687033,\n        -0.90359502, -1.22195475],\n       [-0.63036712,  0.86737544, -0.17119103, ...,  0.01166204,\n        -0.94401292,  1.46828321]])"
  },
  {
    "objectID": "Prediksi_Audio_Emosi.html#modelling",
    "href": "Prediksi_Audio_Emosi.html#modelling",
    "title": "2  Analisis dan Prediksi pada Data Audio Emosi",
    "section": "2.3 Modelling",
    "text": "2.3 Modelling\n\n2.3.1 Mencari k terbaik dengan akurasi paling tinggi\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nK = 30\nacc = np.zeros((K - 1))\n\nfor n in range(1, K, 2):\n    knn = KNeighborsClassifier(n_neighbors=n, metric=\"euclidean\").fit(X_train_scaled, y_train)\n    y_pred = knn.predict(X_test_scaled)\n    acc[n - 1] = accuracy_score(y_test, y_pred)\n\nbest_accuracy = acc.max()\nbest_k = acc.argmax() + 1\n\n# Tampilkan akurasi terbaik dan nilai k\nprint('Akurasi terbaik adalah', best_accuracy, 'dengan nilai k =', best_k)\n\nAkurasi terbaik adalah 0.769406392694064 dengan nilai k = 9\n\n\n\n\n2.3.2 Menyimpan model knn dengan k terbaik\n\n# Simpan model KNN terbaik\nbest_knn = KNeighborsClassifier(n_neighbors=best_k, metric=\"euclidean\")\nbest_knn.fit(X_train_scaled, y_train)\n\nKNeighborsClassifier(metric='euclidean', n_neighbors=9)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(metric='euclidean', n_neighbors=9)\n\n\n\n# Save the best KNN model using pickle\nmodel_file_path = r'model.pkl'\nwith open(model_file_path, 'wb') as model_file:\n    pickle.dump(best_knn, model_file)\n\nwith open(r'model.pkl', 'rb') as knn_model:\n    load_knn = pickle.load(knn_model)\n\n\ny_pred = load_knn.predict(X_test_scaled)\ny_pred\n\narray(['angry', 'fear', 'pleasantsurprised', 'Sad', 'neutral', 'disgust',\n       'pleasantsurprised', 'neutral', 'happy', 'Fear', 'Fear', 'angry',\n       'pleasantsurprised', 'disgust', 'happy', 'pleasantsurprised',\n       'sad', 'neutral', 'sad', 'happy', 'happy', 'disgust', 'Fear',\n       'Sad', 'angry', 'sad', 'sad', 'neutral', 'disgust', 'disgust',\n       'angry', 'pleasantsurprised', 'neutral', 'sad', 'Sad',\n       'pleasantsurprised', 'Fear', 'disgust', 'Fear', 'happy', 'happy',\n       'neutral', 'happy', 'Fear', 'Pleasantsurprise', 'neutral', 'Fear',\n       'disgust', 'disgust', 'sad', 'pleasantsurprised', 'neutral',\n       'happy', 'neutral', 'neutral', 'disgust', 'Fear',\n       'pleasantsurprised', 'sad', 'happy', 'Pleasantsurprise', 'neutral',\n       'Fear', 'Sad', 'pleasantsurprised', 'disgust', 'Sad',\n       'pleasantsurprised', 'disgust', 'Fear', 'Sad', 'Pleasantsurprise',\n       'disgust', 'sad', 'Sad', 'angry', 'Sad', 'happy', 'angry',\n       'neutral', 'sad', 'disgust', 'disgust', 'happy',\n       'pleasantsurprised', 'fear', 'fear', 'neutral', 'Sad', 'neutral',\n       'disgust', 'happy', 'disgust', 'angry', 'Sad', 'happy', 'Sad',\n       'disgust', 'Sad', 'Pleasantsurprise', 'Fear', 'Sad', 'sad', 'Sad',\n       'Sad', 'fear', 'neutral', 'happy', 'Fear', 'disgust', 'fear',\n       'disgust', 'happy', 'Sad', 'sad', 'Pleasantsurprise', 'angry',\n       'happy', 'disgust', 'Pleasantsurprise', 'neutral',\n       'Pleasantsurprise', 'disgust', 'neutral', 'Pleasantsurprise',\n       'pleasantsurprised', 'disgust', 'happy', 'fear', 'sad', 'neutral',\n       'Sad', 'pleasantsurprised', 'happy', 'fear', 'disgust', 'Fear',\n       'Sad', 'neutral', 'happy', 'neutral', 'neutral', 'neutral',\n       'disgust', 'happy', 'Sad', 'neutral', 'disgust', 'disgust',\n       'neutral', 'sad', 'neutral', 'Sad', 'angry', 'Pleasantsurprise',\n       'sad', 'Sad', 'angry', 'happy', 'angry', 'neutral',\n       'pleasantsurprised', 'disgust', 'disgust', 'disgust', 'Fear',\n       'happy', 'Sad', 'Pleasantsurprise', 'neutral', 'neutral', 'angry',\n       'neutral', 'sad', 'sad', 'Fear', 'angry', 'Fear', 'Sad', 'fear',\n       'neutral', 'sad', 'happy', 'disgust', 'sad', 'sad', 'fear',\n       'happy', 'Sad', 'disgust', 'disgust', 'pleasantsurprised',\n       'pleasantsurprised', 'pleasantsurprised', 'Pleasantsurprise',\n       'disgust', 'happy', 'sad', 'angry', 'disgust', 'Fear',\n       'pleasantsurprised', 'pleasantsurprised', 'Fear', 'neutral',\n       'disgust', 'fear', 'disgust', 'pleasantsurprised', 'fear',\n       'disgust', 'Pleasantsurprise', 'pleasantsurprised', 'fear',\n       'neutral', 'angry', 'pleasantsurprised', 'sad', 'Sad', 'happy',\n       'angry', 'happy', 'happy', 'angry', 'angry', 'pleasantsurprised',\n       'neutral', 'sad', 'angry', 'angry', 'pleasantsurprised', 'disgust',\n       'fear', 'disgust', 'happy', 'fear', 'happy', 'Fear', 'sad',\n       'happy', 'Pleasantsurprise', 'neutral', 'disgust', 'happy', 'Sad',\n       'disgust', 'sad', 'disgust', 'neutral', 'angry', 'Fear', 'disgust',\n       'Fear', 'neutral', 'happy', 'neutral', 'angry', 'happy',\n       'pleasantsurprised', 'angry', 'happy', 'fear', 'neutral', 'happy',\n       'Sad', 'happy', 'neutral', 'pleasantsurprised', 'disgust', 'angry',\n       'angry', 'pleasantsurprised', 'Fear', 'pleasantsurprised', 'Fear',\n       'Pleasantsurprise', 'Fear', 'disgust', 'pleasantsurprised',\n       'angry', 'disgust', 'happy', 'happy', 'neutral', 'happy',\n       'neutral', 'Sad', 'Pleasantsurprise', 'happy', 'happy', 'sad',\n       'fear', 'neutral', 'pleasantsurprised', 'disgust', 'Fear',\n       'Pleasantsurprise', 'disgust', 'happy', 'neutral', 'Sad',\n       'disgust', 'fear', 'happy', 'disgust', 'angry', 'disgust', 'happy',\n       'fear', 'disgust', 'neutral', 'Pleasantsurprise', 'neutral',\n       'neutral', 'angry', 'disgust', 'neutral', 'pleasantsurprised',\n       'neutral', 'Sad', 'neutral', 'Fear', 'angry', 'disgust', 'neutral',\n       'happy', 'Sad', 'happy', 'angry', 'disgust', 'neutral', 'happy',\n       'Sad', 'fear', 'sad', 'pleasantsurprised', 'fear',\n       'Pleasantsurprise', 'Fear', 'happy', 'happy', 'neutral',\n       'pleasantsurprised', 'fear', 'Fear', 'Sad', 'Pleasantsurprise',\n       'angry', 'Pleasantsurprise', 'Sad', 'neutral', 'happy', 'Sad',\n       'pleasantsurprised', 'Fear', 'pleasantsurprised', 'sad', 'fear',\n       'neutral', 'neutral', 'happy', 'neutral', 'neutral', 'angry',\n       'Pleasantsurprise', 'disgust', 'neutral', 'fear', 'happy', 'angry',\n       'Fear', 'fear', 'Sad', 'neutral', 'Sad', 'Fear', 'disgust',\n       'happy', 'disgust', 'neutral', 'Sad', 'Pleasantsurprise', 'sad',\n       'fear', 'happy', 'neutral', 'happy', 'happy', 'happy', 'neutral',\n       'neutral', 'neutral', 'disgust', 'happy', 'Sad', 'disgust', 'Fear',\n       'neutral', 'angry', 'pleasantsurprised', 'pleasantsurprised',\n       'happy', 'angry', 'disgust', 'pleasantsurprised',\n       'pleasantsurprised', 'neutral', 'pleasantsurprised',\n       'pleasantsurprised', 'angry', 'Sad', 'disgust',\n       'pleasantsurprised', 'sad', 'Fear', 'neutral', 'angry',\n       'pleasantsurprised', 'happy', 'Sad', 'sad', 'neutral', 'disgust',\n       'Pleasantsurprise', 'pleasantsurprised', 'neutral', 'angry',\n       'happy', 'disgust', 'angry', 'angry', 'neutral', 'happy', 'fear',\n       'disgust', 'pleasantsurprised', 'Sad', 'sad'], dtype=object)\n\n\n\nprint('Akurasi KNN dengan data test:')\naccuracy = accuracy_score(y_test, y_pred)\nprint( accuracy)\n\nAkurasi KNN dengan data test:\n0.769406392694064\n\n\n\n# Hitung prediksi label KNN\nknn_predictions = load_knn.predict(X_test_scaled)\n\n# Simpan hasil prediksi KNN ke dalam DataFrame\nknn_results_df = pd.DataFrame({'Actual Label': y_test, 'Predicted Label (KNN)': knn_predictions})\n\n# Tampilkan tabel prediksi KNN\nprint(\"Tabel Prediksi Label KNN\")\nknn_results_df\n\nTabel Prediksi Label KNN\n\n\n\n  \n    \n\n\n\n\n\n\nActual Label\nPredicted Label (KNN)\n\n\n\n\n353\nangry\nangry\n\n\n1631\nfear\nfear\n\n\n414\npleasantsurprised\npleasantsurprised\n\n\n1827\nneutral\nSad\n\n\n851\nneutral\nneutral\n\n\n...\n...\n...\n\n\n1613\nfear\nfear\n\n\n50\ndisgust\ndisgust\n\n\n453\npleasantsurprised\npleasantsurprised\n\n\n1822\nneutral\nSad\n\n\n1233\nsad\nsad\n\n\n\n\n\n438 rows × 2 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n2.3.3 Reduksi PCA\nReduksi PCA (Principal Component Analysis) adalah teknik yang digunakan untuk mengurangi dimensi dari dataset yang kompleks, dengan tetap mempertahankan sebagian besar informasi yang terkandung dalam dataset tersebut. PCA bekerja dengan mentransformasi data asli ke dalam ruang fitur baru yang disebut komponen utama atau principal components. Komponen utama ini diurutkan berdasarkan seberapa besar variansinya, sehingga komponen pertama menyimpan sebagian besar varians, yang diikuti oleh komponen kedua, dan seterusnya.\n\n# Lakukan reduksi PCA\nsklearn_pca = sklearnPCA(n_components=10)\nX_train_pca = sklearn_pca.fit_transform(X_train_scaled)\n\nprint(\"Principal Components 10:\")\nX_train_pca\n\nPrincipal Components 10:\n\n\narray([[-2.15699059,  0.12088686, -1.05816054, ...,  0.15522723,\n        -0.09174985, -0.01146861],\n       [ 2.70852347, -0.64630269,  0.05224175, ...,  0.00458544,\n        -0.04659703, -0.08161985],\n       [ 2.91065674,  0.09092699, -0.62907659, ...,  0.05744571,\n        -0.00876878, -0.01138743],\n       ...,\n       [ 1.00230967, -0.72522836,  2.26887371, ..., -0.10442859,\n         0.15861958, -0.09644085],\n       [ 3.10754228, -0.69602145,  0.38281969, ...,  0.05710673,\n         0.06719287,  0.07069764],\n       [-2.00722733,  0.50821046,  1.9109502 , ..., -0.00369189,\n        -0.05388441,  0.01722397]])\n\n\n\n# Save the PCA model\npca_model_file_path = r'PCA10.pkl'\nwith open(pca_model_file_path, 'wb') as pca_model_file:\n    pickle.dump(sklearn_pca, pca_model_file)\n# Load the PCA model\nwith open(pca_model_file_path, 'rb') as pca_model:\n    loadpca = pickle.load(pca_model)\n\n\n# Transform test data using the loaded PCA model\nX_test_pca = loadpca.transform(X_test_scaled)\n\n# Continue with KNN and evaluation as needed\nK = 30\nacc_pca = np.zeros((K - 1))\nfor n in range(1, K, 2):\n    knn_pca = KNeighborsClassifier(n_neighbors=n, metric=\"euclidean\").fit(X_train_pca, y_train)\n    y_pred_pca = knn_pca.predict(X_test_pca)\n    acc_pca[n - 1] = accuracy_score(y_test, y_pred_pca)\n\nbest_accuracy_pca = acc_pca.max()\nbest_k_pca = acc_pca.argmax() + 1\n\n# Tampilkan akurasi terbaik dan nilai k dengan PCA\nprint('Akurasi KNN terbaik dengan PCA adalah', best_accuracy_pca, 'dengan nilai k =', best_k_pca)\n\nAkurasi KNN terbaik dengan PCA adalah 0.769406392694064 dengan nilai k = 9\n\n\n\n# Hitung prediksi label KNN setelah PCA\nknn_pca_predictions = knn_pca.predict(X_test_pca)\n\n# Simpan hasil prediksi KNN setelah PCA ke dalam DataFrame\nknn_pca_results_df = pd.DataFrame({'Actual Label': y_test, 'Predicted Label (KNN with PCA)': knn_pca_predictions})\n\n# Tampilkan tabel prediksi KNN setelah PCA\nprint(\"Tabel Prediksi Label KNN dengan PCA\")\nknn_pca_results_df\n\nTabel Prediksi Label KNN dengan PCA\n\n\n\n  \n    \n\n\n\n\n\n\nActual Label\nPredicted Label (KNN with PCA)\n\n\n\n\n353\nangry\nangry\n\n\n1631\nfear\nfear\n\n\n414\npleasantsurprised\nneutral\n\n\n1827\nneutral\nSad\n\n\n851\nneutral\nneutral\n\n\n...\n...\n...\n\n\n1613\nfear\nfear\n\n\n50\ndisgust\ndisgust\n\n\n453\npleasantsurprised\npleasantsurprised\n\n\n1822\nneutral\nneutral\n\n\n1233\nsad\nsad\n\n\n\n\n\n438 rows × 2 columns"
  },
  {
    "objectID": "Prediksi_Audio_Emosi.html#evaluasi",
    "href": "Prediksi_Audio_Emosi.html#evaluasi",
    "title": "2  Analisis dan Prediksi pada Data Audio Emosi",
    "section": "2.4 Evaluasi",
    "text": "2.4 Evaluasi\n\n2.4.1 Prediksi Data Audio\n\nimport librosa\nimport numpy as np\n\n# Fungsi untuk menghitung fitur audio\ndef extract_features(audio_path):\n    y, sr = librosa.load(audio_path)\n\n    # Fitur 1: Mean\n    mean = np.mean(y)\n\n    # Fitur 2: Median\n    median = np.median(y)\n\n    # Fitur 3: Standard Deviation\n    std_deviation = np.std(y)\n\n    # Fitur 4: Zero Crossing Rate\n    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))\n\n    # Fitur 5: Energy\n    energy = np.mean(librosa.feature.rms(y=y))\n\n    # Fitur 6: Spectral Centroid\n    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n\n    # Fitur 7: Spectral Bandwidth\n    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n\n    # Fitur 8: Spectral Roll-off\n    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n\n    # Fitur 9: Chroma Feature\n    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n\n    # Fitur 10: Mel-frequency Cepstral Coefficients (MFCCs)\n    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13))\n\n    return [mean, median, std_deviation, zero_crossing_rate, energy,\n            spectral_centroid, spectral_bandwidth, spectral_rolloff, chroma, mfccs]\n\nprint('Ekstraksi Fitur Audio')\nprint('Unggah file audio WAV untuk menghitung fitur statistiknya.')\n\n# Unggah file audio\nfrom google.colab import files\nuploaded_audio = files.upload()\n\nif uploaded_audio:\n    audio_path = list(uploaded_audio.keys())[0]\n    print(f\"Audio file: {audio_path}\")\n\n    audio_features = extract_features(audio_path)\n\n    feature_names = [\n        \"Mean\", \"Median\", \"Std Deviation\", \"Zero Crossing Rate\", \"Energy\",\n        \"Spectral Centroid\", \"Spectral Bandwidth\", \"Spectral Rolloff\", \"Chroma\", \"MFCCs\"\n    ]\n\n    # Tampilkan hasil fitur\n    print(\"### Hasil Ekstraksi Fitur Audio:\")\n    for i, feature in enumerate(audio_features):\n        print(f\"{feature_names[i]}: {feature}\")\n\n    # Transform audio_features using the loaded scaler\n    # (Pastikan loadscal, loadpca, dan load_knn sudah di-load sebelumnya)\n    datauji = loadscal.transform(np.array(audio_features).reshape(1, -1))\n    datapca = loadpca.transform(datauji)\n    # Make predictions using the KNN model\n    y_pred_uji = load_knn.predict(datauji)\n    # y_pred_pca = load_knn.predict(datapca)\n\n    print(\"Fitur-fitur setelah di normalisasi: \", datauji)\n    print(\"Data PCA:\", datapca)\n    # print(\"Predicted Label (PCA):\", y_pred_pca)\n    print(\"Predicted Label (KNN):\", y_pred_uji)\n\nEkstraksi Fitur Audio\nUnggah file audio WAV untuk menghitung fitur statistiknya.\n\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving OAF_bath_sad.wav to OAF_bath_sad.wav\nAudio file: OAF_bath_sad.wav\n### Hasil Ekstraksi Fitur Audio:\nMean: -4.950474249199033e-06\nMedian: 0.0005449416348710656\nStd Deviation: 0.013166016899049282\nZero Crossing Rate: 0.07502528599330358\nEnergy: 0.010921107605099678\nSpectral Centroid: 1823.6221088038858\nSpectral Bandwidth: 2179.453805174795\nSpectral Rolloff: 3287.8509521484375\nChroma: 0.3211115300655365\nMFCCs: -30.506229400634766\nFitur-fitur setelah di normalisasi:  [[ 0.27547308  0.03367706 -0.99734606 -1.38292082 -1.01350426 -1.3458874\n  -0.13268214 -1.26206853  0.4160591  -0.45044388]]\nData PCA: [[-2.67397594e+00  1.15609678e-01 -2.55621792e-01 -1.74982840e-02\n   6.99985866e-01 -2.89191174e-01 -7.49915711e-02 -1.24858203e-02\n  -1.94857090e-04  2.58036094e-02]]\nPredicted Label (KNN): ['Sad']\n\n\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n  warnings.warn("
  },
  {
    "objectID": "prediksi_pycaret.html#business-understanding",
    "href": "prediksi_pycaret.html#business-understanding",
    "title": "3  Analisis dan Prediksi pada dataset Heart failure clinical records Menggunakan Pycaret.",
    "section": "3.1 Business understanding",
    "text": "3.1 Business understanding\n\n3.1.1 Tujuan Analisis:\nAnalisis pada dataset Heart failure clinical records bertujuan untuk memprediksi kemungkinan terjadinya gagal jantung pada pasien berdasarkan 13 fitur-fitur klinis mereka."
  },
  {
    "objectID": "prediksi_pycaret.html#data-understanding-memahami-data",
    "href": "prediksi_pycaret.html#data-understanding-memahami-data",
    "title": "3  Analisis dan Prediksi pada dataset Heart failure clinical records Menggunakan Pycaret.",
    "section": "3.2 Data understanding / Memahami Data",
    "text": "3.2 Data understanding / Memahami Data\n\n3.2.1 Deskripsi Dataset\nDataset “Heart failure clinical records” merupakan kumpulan data yang berisi catatan medis dari 299 pasien yang mengalami gagal jantung. Data ini dikumpulkan selama periode pemantauan pasien-pasien tersebut. Setiap profil pasien dalam dataset ini dilengkapi dengan 13 fitur klinis yang mencerminkan kondisi kesehatan mereka. Di bawah ini, saya akan menjelaskan deskripsi dari dataset ini serta tujuan utamanya:\n\n3.2.1.1 Deskripsi Dataset Dan Tujuan Dataset:\n\nJumlah Sampel: Dataset ini berisi informasi dari 299 pasien yang mengalami gagal jantung dan tidak ada missing values.\nFitur Klinis: Setiap pasien dalam dataset ini memiliki 13 fitur klinis yang mencakup berbagai aspek dari kesehatan mereka. Beberapa contoh fitur klinis yang mungkin termasuk dalam dataset ini adalah Usia, tekanan darah, kadar serum kreatinin, kadar serum natrium, kadar serum kalium, ejection fraction (fraksi ejeksi), jenis kelamin pasien, dan lain sebagainya. dibawah ini adalah fitur pada dataset: ‘age’, ‘anaemia’, ‘creatinine_phosphokinase’, ‘diabetes’, ‘ejection_fraction’, ‘high_blood_pressure’, ‘platelets’, ‘serum_creatinine’, ‘serum_sodium’, ‘sex’, ‘smoking’, ‘time’, ‘DEATH_EVENT’\nDataset ini memiliki target pada kolom Death Event. Fitur target “death event” adalah sebuah fitur yang digunakan untuk menunjukkan apakah pasien mengalami kematian selama periode pemantauan atau tidak. Fitur ini bersifat boolean, yang berarti nilainya hanya dapat berupa dua kemungkinan: 1 / True (benar) atau 0 / False (salah). Jumlah Target dengan nilai 1 Berjumlah 96 dan nilai 0 berjumlah 203. jumlah antara setiap target tidak seimbang maka pada processing akan dilakukan oversampling untuk menyeimbangkan setiap target.\nTujuan Dataset: Dataset ini memiliki beberapa tujuan utama, antara lain:\n\nAnalisis dan Penelitian Kesehatan: Data ini dapat digunakan untuk menganalisis faktor-faktor risiko dan prediksi gagal jantung, serta untuk memahami hubungan antara berbagai fitur klinis dan kondisi pasien.\nPengembangan Model Prediktif: Dataset ini dapat digunakan untuk mengembangkan model prediktif yang dapat memprediksi kemungkinan terjadinya gagal jantung pada pasien berdasarkan fitur-fitur klinis mereka. Hal ini dapat membantu tenaga medis dalam melakukan tindakan pencegahan yang lebih tepat waktu.\n\n\nTujuan utama dari dataset ini adalah meningkatkan pemahaman tentang gagal jantung, membantu dalam pengembangan metode prediktif, serta mendukung penelitian dan pengembangan terkait kesehatan jantung. Data ini menjadi dasar penting untuk menjalankan berbagai analisis dan penelitian dalam upaya untuk meningkatkan diagnosis, perawatan, dan pencegahan penyakit gagal jantung.\n\n# Import Library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pycaret.classification import *\nfrom imblearn.over_sampling import RandomOverSampler\nfrom pycaret.classification import *\nimport pickle\n\n\n# Baca data dari file CSV yang sudah di upload di github\nurl = \"https://raw.githubusercontent.com/whintaaa/datapsd/main/heart_failure_clinical_records_dataset.csv\"\ndf = pd.read_csv(url)\n# menampilkan dataset\ndf\n\n\n  \n    \n\n\n\n\n\n\nage\nanaemia\ncreatinine_phosphokinase\ndiabetes\nejection_fraction\nhigh_blood_pressure\nplatelets\nserum_creatinine\nserum_sodium\nsex\nsmoking\ntime\nDEATH_EVENT\n\n\n\n\n0\n75.0\n0\n582\n0\n20\n1\n265000.00\n1.9\n130\n1\n0\n4\n1\n\n\n1\n55.0\n0\n7861\n0\n38\n0\n263358.03\n1.1\n136\n1\n0\n6\n1\n\n\n2\n65.0\n0\n146\n0\n20\n0\n162000.00\n1.3\n129\n1\n1\n7\n1\n\n\n3\n50.0\n1\n111\n0\n20\n0\n210000.00\n1.9\n137\n1\n0\n7\n1\n\n\n4\n65.0\n1\n160\n1\n20\n0\n327000.00\n2.7\n116\n0\n0\n8\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n294\n62.0\n0\n61\n1\n38\n1\n155000.00\n1.1\n143\n1\n1\n270\n0\n\n\n295\n55.0\n0\n1820\n0\n38\n0\n270000.00\n1.2\n139\n0\n0\n271\n0\n\n\n296\n45.0\n0\n2060\n1\n60\n0\n742000.00\n0.8\n138\n0\n0\n278\n0\n\n\n297\n45.0\n0\n2413\n0\n38\n0\n140000.00\n1.4\n140\n1\n1\n280\n0\n\n\n298\n50.0\n0\n196\n0\n45\n0\n395000.00\n1.6\n136\n1\n1\n285\n0\n\n\n\n\n\n299 rows × 13 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n\n3.2.1.2 Penjelasan Fitur-fitur:\n\n# Print the column names to identify the correct target variable\nprint(df.columns)\n\nIndex(['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time',\n       'DEATH_EVENT'],\n      dtype='object')\n\n\nFitur-fitur klinis dalam dataset “Heart Failure Clinical Records” adalah sebagai berikut:\n\nUsia (age): Ini adalah usia pasien dalam tahun. Fitur ini memberikan informasi tentang berapa usia pasien yang mengalami gagal jantung. Usia seringkali menjadi faktor penting dalam menilai risiko dan prognosis penyakit jantung. Fitur ini bertipe data numerik.\nAnemia (anaemia): Ini adalah fitur boolean yang menunjukkan apakah pasien mengalami penurunan jumlah sel darah merah atau kadar hemoglobin. Nilai 1 / “true” menunjukkan kehadiran anemia, sementara 0 / “false” menunjukkan ketiadaan anemia.\nKreatinin Fosfokinase (CPK): Ini adalah tingkat enzim CPK dalam darah, diukur dalam mikrogram per liter (mcg/L). Tingkat CPK dalam darah dapat memberikan indikasi adanya kerusakan otot atau jaringan jantung. Ini adalah indikator penting dalam penilaian kondisi jantung. Fitur ini bertipe data numerik.\nDiabetes: Ini adalah fitur boolean yang menunjukkan apakah pasien menderita diabetes atau tidak. Nilai 1 / “true” menunjukkan keberadaan diabetes, sementara 0 / “false” menunjukkan ketiadaan diabetes. Diabetes merupakan faktor risiko yang signifikan dalam perkembangan penyakit jantung.\nFraksi Ejeksi (Ejection Fraction): Ini adalah persentase darah yang meninggalkan jantung pada setiap kontraksi. Fraksi ejeksi ini dinyatakan dalam persentase. Ini adalah ukuran penting dalam menilai kemampuan jantung untuk memompa darah dan dapat memberikan informasi tentang fungsi jantung. Fitur ini bertipe data numerik.\nTekanan Darah Tinggi (High Blood Pressure): Ini adalah fitur boolean yang menunjukkan apakah pasien memiliki hipertensi atau tidak. Nilai 1 / “true” menunjukkan keberadaan tekanan darah tinggi, sementara 2 / “false” menunjukkan ketiadaan tekanan darah tinggi. Tekanan darah tinggi adalah faktor risiko utama untuk penyakit jantung.\nPlatelet (platelets): Ini adalah jumlah platelet dalam darah, diukur dalam ribu platelet per mililiter (kiloplatelets/mL). Platelet adalah sel darah yang berperan dalam pembekuan darah. Nilai platelet dalam darah dapat memberikan informasi tentang kemampuan darah untuk membeku. Fitur ini bertipe data numerik.\nJenis Kelamin (Sex): Ini adalah fitur biner yang menunjukkan jenis kelamin pasien, yaitu perempuan (woman) atau laki-laki (man). Informasi ini dapat digunakan untuk mengevaluasi perbedaan jenis kelamin dalam insiden gagal jantung.\nKreatinin Serum (Serum Creatinine): Ini adalah tingkat kreatinin serum dalam darah, diukur dalam miligram per desiliter (mg/dL). Kreatinin adalah produk sisa metabolisme yang dapat memberikan informasi tentang fungsi ginjal. Tingkat kreatinin serum yang tinggi dapat menunjukkan masalah ginjal yang dapat mempengaruhi kondisi jantung. Fitur ini bertipe data numerik.\nNatrium Serum (Serum Sodium): Ini adalah tingkat natrium serum dalam darah, diukur dalam miliequivalents per liter (mEq/L). Natrium adalah elektrolit penting dalam tubuh dan tingkat natrium serum dapat memberikan informasi tentang keseimbangan elektrolit yang dapat mempengaruhi fungsi jantung. Fitur ini bertipe data numerik.\nMerokok (Smoking): Ini adalah fitur biner yang menunjukkan apakah pasien merokok atau tidak. Nilai 1 / “true” menunjukkan kebiasaan merokok, sementara 0 / “false” menunjukkan ketiadaan kebiasaan merokok. Merokok adalah faktor risiko yang signifikan dalam perkembangan penyakit jantung.\nWaktu (Time): Ini adalah periode pemantauan atau follow-up pasien dalam satuan hari (days). Fitur ini mengukur lamanya pasien dipantau dalam penelitian. Ini adalah informasi penting dalam analisis klinis dan penelitian lanjutan. Fitur ini bertipe data numerik.\nKejadian Kematian (Death Event): Fitur target “death event” adalah sebuah fitur yang digunakan untuk menunjukkan apakah pasien mengalami kematian selama periode pemantauan atau tidak. Fitur ini bersifat boolean, yang berarti nilainya hanya dapat berupa dua kemungkinan: 1 / True (benar) atau 0 / False (salah). Jika “death event” memiliki nilai True, ini berarti bahwa pasien tersebut mengalami kematian selama periode pemantauan yang dicatat dalam dataset. Sebaliknya, jika “death event” memiliki nilai False, maka ini menunjukkan bahwa pasien tersebut masih hidup atau tidak mengalami kematian selama periode tersebut.\n\n\n\n\n3.2.2 Eksplorasi Dataset / Satistik Dataset:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Baca data dari URL CSV\nurl = \"https://raw.githubusercontent.com/whintaaa/datapsd/main/heart_failure_clinical_records_dataset.csv\"\ndata = pd.read_csv(url)\n\n\n# Informasi Umum tentang Dataset\ninfo = data.info()\n\n# Statistik Deskriptif untuk Kolom-Kolom Numerik\ndescribe = data.describe()\n\n# Jumlah Nilai yang Hilang untuk Setiap Kolom\nmissing_values = data.isnull().sum()\n\n# Beberapa Baris Pertama dari Dataset\nhead = data.head()\n\n# Jumlah Unik untuk Kolom Target (DEATH_EVENT)\ntarget_counts = data['DEATH_EVENT'].value_counts()\n\n# Korelasi Antar Kolom Numerik\ncorrelation_matrix = data.corr()\n\n# Distribusi Umur (Age)\nage_distribution = data['age'].value_counts()\n\n# Visualisasi Data\nplt.figure(figsize=(15, 10))\n\n# Countplot untuk Kolom Target (DEATH_EVENT)\nplt.figure(figsize=(8, 5))\nsns.countplot(x='DEATH_EVENT', data=data)\nplt.title('Countplot untuk Kolom Target (DEATH_EVENT)')\nplt.xlabel('DEATH_EVENT')\nplt.ylabel('Jumlah')\nplt.show()\n\n# Grafik distribusi umur (Age)\nplt.figure(figsize=(8, 6))\nsns.histplot(data['age'], bins=30, kde=True, color='skyblue')\nplt.title('Distribusi Umur')\nplt.xlabel('Umur')\nplt.ylabel('Frekuensi')\nplt.show()\n\n# Grafik korelasi antara umur (Age) dan kadar serum kreatinin (Serum Creatinine)\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x='age', y='serum_creatinine', data=data, hue='sex', palette='viridis')\nplt.title('Korelasi Umur dan Kadar Serum Kreatinin')\nplt.xlabel('Umur')\nplt.ylabel('Kadar Serum Kreatinin')\nplt.legend(title='Jenis Kelamin', loc='upper right', labels=['Female', 'Male'])\nplt.show()\n\n# Contoh menampilkan hasil\nprint(\"Informasi Umum tentang Dataset:\")\nprint(info)\n\nprint(\"\\nJumlah Nilai yang Hilang:\")\nprint(missing_values)\n\nprint(\"\\nJumlah data untuk Kolom Target (DEATH_EVENT):\")\nprint(target_counts)\n\n# Statistik deskriptif\nstatistics = data.describe()\n\n# Menampilkan hasil\nprint(\"\\nStatistik Deskriptif:\")\nprint(statistics)\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 299 entries, 0 to 298\nData columns (total 13 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   age                       299 non-null    float64\n 1   anaemia                   299 non-null    int64  \n 2   creatinine_phosphokinase  299 non-null    int64  \n 3   diabetes                  299 non-null    int64  \n 4   ejection_fraction         299 non-null    int64  \n 5   high_blood_pressure       299 non-null    int64  \n 6   platelets                 299 non-null    float64\n 7   serum_creatinine          299 non-null    float64\n 8   serum_sodium              299 non-null    int64  \n 9   sex                       299 non-null    int64  \n 10  smoking                   299 non-null    int64  \n 11  time                      299 non-null    int64  \n 12  DEATH_EVENT               299 non-null    int64  \ndtypes: float64(3), int64(10)\nmemory usage: 30.5 KB\n\n\n&lt;Figure size 1500x1000 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\nInformasi Umum tentang Dataset:\nNone\n\nJumlah Nilai yang Hilang:\nage                         0\nanaemia                     0\ncreatinine_phosphokinase    0\ndiabetes                    0\nejection_fraction           0\nhigh_blood_pressure         0\nplatelets                   0\nserum_creatinine            0\nserum_sodium                0\nsex                         0\nsmoking                     0\ntime                        0\nDEATH_EVENT                 0\ndtype: int64\n\nJumlah data untuk Kolom Target (DEATH_EVENT):\n0    203\n1     96\nName: DEATH_EVENT, dtype: int64\n\nStatistik Deskriptif:\n              age     anaemia  creatinine_phosphokinase    diabetes  \\\ncount  299.000000  299.000000                299.000000  299.000000   \nmean    60.833893    0.431438                581.839465    0.418060   \nstd     11.894809    0.496107                970.287881    0.494067   \nmin     40.000000    0.000000                 23.000000    0.000000   \n25%     51.000000    0.000000                116.500000    0.000000   \n50%     60.000000    0.000000                250.000000    0.000000   \n75%     70.000000    1.000000                582.000000    1.000000   \nmax     95.000000    1.000000               7861.000000    1.000000   \n\n       ejection_fraction  high_blood_pressure      platelets  \\\ncount         299.000000           299.000000     299.000000   \nmean           38.083612             0.351171  263358.029264   \nstd            11.834841             0.478136   97804.236869   \nmin            14.000000             0.000000   25100.000000   \n25%            30.000000             0.000000  212500.000000   \n50%            38.000000             0.000000  262000.000000   \n75%            45.000000             1.000000  303500.000000   \nmax            80.000000             1.000000  850000.000000   \n\n       serum_creatinine  serum_sodium         sex    smoking        time  \\\ncount         299.00000    299.000000  299.000000  299.00000  299.000000   \nmean            1.39388    136.625418    0.648829    0.32107  130.260870   \nstd             1.03451      4.412477    0.478136    0.46767   77.614208   \nmin             0.50000    113.000000    0.000000    0.00000    4.000000   \n25%             0.90000    134.000000    0.000000    0.00000   73.000000   \n50%             1.10000    137.000000    1.000000    0.00000  115.000000   \n75%             1.40000    140.000000    1.000000    1.00000  203.000000   \nmax             9.40000    148.000000    1.000000    1.00000  285.000000   \n\n       DEATH_EVENT  \ncount    299.00000  \nmean       0.32107  \nstd        0.46767  \nmin        0.00000  \n25%        0.00000  \n50%        0.00000  \n75%        1.00000  \nmax        1.00000"
  },
  {
    "objectID": "prediksi_pycaret.html#data-prepocessing",
    "href": "prediksi_pycaret.html#data-prepocessing",
    "title": "3  Analisis dan Prediksi pada dataset Heart failure clinical records Menggunakan Pycaret.",
    "section": "3.3 Data Prepocessing",
    "text": "3.3 Data Prepocessing\n\n3.3.1 Oversampling\nMelihat jumlah masing-masing target pada kolom ‘death event’:\n\n# Menghitung jumlah masing-masing target pada kolom 'death event'\njumlah_death_event_1 = df[df['DEATH_EVENT'] == 1].shape[0]\njumlah_death_event_0 = df[df['DEATH_EVENT'] == 0].shape[0]\n\n# Menampilkan jumlah masing-masing target\nprint(\"Jumlah Target 'death event' dengan Nilai 1:\", jumlah_death_event_1)\nprint(\"Jumlah Target 'death event' dengan Nilai 0:\", jumlah_death_event_0)\n\nJumlah Target 'death event' dengan Nilai 1: 96\nJumlah Target 'death event' dengan Nilai 0: 203\n\n\nBisa dilihat jumlah target dengan nilai 1 = 96 dan nilai 0 = 203 ini menandakan bahwa jumlah target pada dataset tidak seimbang. Maka salah satu metode untuk menyeimbangkan target bisa menggunakan teknik oversampling. Teknik oversampling adalah salah satu pendekatan untuk menyeimbangkan dataset yang tidak seimbang dengan meningkatkan jumlah sampel dalam kategori minoritas. Kategori minoritas adalah kelas target yang memiliki frekuensi yang lebih rendah dibandingkan dengan kelas mayoritas. Teknik oversampling dilakukan dengan cara menambahkan lebih banyak contoh dari kategori minoritas agar jumlahnya sebanding dengan kategori mayoritas.\nAda beberapa metode oversampling yang umum digunakan, dan salah satunya adalah RandomOverSampler. Dalam RandomOverSampler, sampel acak dari kategori minoritas ditambahkan kembali ke dataset hingga jumlahnya setara dengan jumlah sampel dalam kategori mayoritas.\nBerikut adalah langkah-langkah umum untuk menggunakan teknik oversampling:\n\nIdentifikasi dataset yang tidak seimbang.\nPisahkan fitur (X) dan target (y).\nTerapkan teknik oversampling pada kategori minoritas.\nGabungkan kembali data yang sudah diresampling.\nLanjutkan dengan analisis atau pemodelan seperti biasa.\n\nDengan menggunakan teknik oversampling, kita meningkatkan jumlah sampel di kategori minoritas (DEATH_EVENT = 1) sehingga seimbang dengan kategori mayoritas (DEATH_EVENT = 0).\n\n# Print the column names to identify the correct target variable\nprint(df.columns)\n\n# Pilih kolom-kolom yang perlu dinormalisasi / bertype numerik\nnumerical_columns = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium']\n\n# Langkah 3: Split data menjadi fitur (X) dan target (y)\nX = df.drop(columns=['DEATH_EVENT'])\ny = df['DEATH_EVENT']\n\n# Menggunakan teknik oversampling dengan RandomOverSampler\noversampler = RandomOverSampler(random_state=42)\nX_resampled, y_resampled = oversampler.fit_resample(X, y)\n\n# Membuat dataframe baru setelah oversampling\ndf_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n\n# Menampilkan jumlah target setelah oversampling\nprint(\"Jumlah Target setelah Oversampling:\")\nprint(df_resampled['DEATH_EVENT'].value_counts())\n\nIndex(['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time',\n       'DEATH_EVENT'],\n      dtype='object')\nJumlah Target setelah Oversampling:\n1    203\n0    203\nName: DEATH_EVENT, dtype: int64\n\n\n\n\n3.3.2 Normalisasi Menggunakan Zscore\nNormalisasi Z-score adalah teknik normalisasi yang digunakan untuk mengubah setiap nilai dalam suatu variabel ke dalam skala yang memiliki rata-rata nol dan deviasi standar satu. Ini adalah cara umum untuk menormalkan data sehingga nilai-nilai yang berbeda dari variabel yang sama dapat dibandingkan secara langsung.\nProses normalisasi Z-score melibatkan mengurangkan rata-rata dari setiap nilai dalam variabel dan membaginya dengan deviasi standar. Formula normalisasi Z-score untuk suatu nilai (x) dalam variabel (X) adalah sebagai berikut:\n\\([ z = \\frac{{x - \\text{{mean}}(X)}}{{\\text{{std}}(X)}} ]\\)\ndi mana: - $( z $) adalah nilai hasil normalisasi (Z-score) dari \\((x\\)). - $( (X) $) adalah rata-rata dari variabel \\((X\\)). - $( (X) $) adalah deviasi standar dari variabel \\((X\\)).\nProses ini menghasilkan distribusi data yang memiliki rata-rata nol dan deviasi standar satu. Normalisasi Z-score sangat berguna dalam beberapa konteks, terutama ketika Anda ingin membandingkan nilai-nilai dari variabel yang memiliki skala yang berbeda.\nsebelum di mormalisasi terdapat fitur boolean pada dataset maka harus dipidahkan dengan fitur numerik karena hanya fitur numerik yang akan di normalisasi kemudian normalisasi Z-score diaktifkan dengan parameter normalize=True dan normalize_method='zscore'. PyCaret akan otomatis menormalisasi fitur-fitur numerik yang ditentukan menggunakan normalisasi Z-score. Lalu secara default pycaret akan membagi dataset menjadi 70% data train dan 30% data test, maka untuk dataset ini 209 menjadi data train dan 90 menjadi data test.\n\n# Print the column names to identify the correct target variable\nprint(df.columns)\n# Pilih kolom-kolom yang perlu dinormalisasi / bertype numerik\nnumerical_columns = ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine', 'serum_sodium']\n\n# Split data menjadi fitur (X) dan target (y)\nX = df.drop(columns=['DEATH_EVENT'])\ny = df['DEATH_EVENT']\n\n# Inisialisasi ekperimen PyCaret\nexp = setup(data=df, target='DEATH_EVENT', normalize=True, normalize_method='zscore', numeric_features=numerical_columns)\n\nIndex(['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time',\n       'DEATH_EVENT'],\n      dtype='object')\n\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n8772\n\n\n1\nTarget\nDEATH_EVENT\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(299, 13)\n\n\n4\nTransformed data shape\n(299, 13)\n\n\n5\nTransformed train set shape\n(209, 13)\n\n\n6\nTransformed test set shape\n(90, 13)\n\n\n7\nNumeric features\n6\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nNormalize\nTrue\n\n\n13\nNormalize method\nzscore\n\n\n14\nFold Generator\nStratifiedKFold\n\n\n15\nFold Number\n10\n\n\n16\nCPU Jobs\n-1\n\n\n17\nUse GPU\nFalse\n\n\n18\nLog Experiment\nFalse\n\n\n19\nExperiment Name\nclf-default-name\n\n\n20\nUSI\n1259"
  },
  {
    "objectID": "prediksi_pycaret.html#modelling",
    "href": "prediksi_pycaret.html#modelling",
    "title": "3  Analisis dan Prediksi pada dataset Heart failure clinical records Menggunakan Pycaret.",
    "section": "3.4 Modelling",
    "text": "3.4 Modelling\n\n3.4.1 Mencari Model Terbaik Menggunakan Pycaret\nPycaret adalah library Python yang menyederhanakan proses pengembangan model machine learning. Dengan fitur otomatis seperti setup data, pemilihan model, optimasi hyperparameter, dan visualisasi hasil, Pycaret memungkinkan pengguna untuk fokus pada inti pemodelan tanpa menulis banyak kode. Berikut implementasi pycaret untuk mencari model terbaik untuk memprediksi resiko gagal jantung pada pasien:\n\n# Bandingkan model dan cari yang terbaik\nbest_model = compare_models()\n\nbest_model\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\nrf\nRandom Forest Classifier\n0.8274\n0.8750\n0.6500\n0.8216\n0.6962\n0.5805\n0.6064\n0.4490\n\n\nlr\nLogistic Regression\n0.8131\n0.8522\n0.6476\n0.7370\n0.6703\n0.5442\n0.5588\n0.1340\n\n\nridge\nRidge Classifier\n0.8131\n0.0000\n0.6476\n0.7370\n0.6703\n0.5442\n0.5588\n0.0800\n\n\nlda\nLinear Discriminant Analysis\n0.8131\n0.8535\n0.6476\n0.7370\n0.6703\n0.5442\n0.5588\n0.0660\n\n\ngbc\nGradient Boosting Classifier\n0.8129\n0.8676\n0.6762\n0.7685\n0.6904\n0.5589\n0.5808\n0.1400\n\n\nxgboost\nExtreme Gradient Boosting\n0.7938\n0.8795\n0.6786\n0.7238\n0.6787\n0.5290\n0.5449\n0.1310\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.7938\n0.8655\n0.6643\n0.7437\n0.6740\n0.5257\n0.5457\n0.2710\n\n\net\nExtra Trees Classifier\n0.7843\n0.8326\n0.4976\n0.7825\n0.5760\n0.4448\n0.4792\n0.3380\n\n\nada\nAda Boost Classifier\n0.7700\n0.8135\n0.5500\n0.7420\n0.5950\n0.4430\n0.4740\n0.1260\n\n\ndt\nDecision Tree Classifier\n0.7650\n0.7179\n0.6000\n0.7023\n0.5992\n0.4397\n0.4670\n0.1620\n\n\nnb\nNaive Bayes\n0.7462\n0.7793\n0.4881\n0.6703\n0.5337\n0.3714\n0.3930\n0.1850\n\n\nqda\nQuadratic Discriminant Analysis\n0.7367\n0.7423\n0.4429\n0.6314\n0.5058\n0.3382\n0.3544\n0.0410\n\n\nsvm\nSVM - Linear Kernel\n0.7314\n0.0000\n0.6214\n0.5955\n0.5924\n0.3960\n0.4105\n0.1030\n\n\nknn\nK Neighbors Classifier\n0.7124\n0.7169\n0.2524\n0.5750\n0.3261\n0.2046\n0.2372\n0.1880\n\n\ndummy\nDummy Classifier\n0.6795\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0360\n\n\n\n\n\n\n\n\n\n\n\nRandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='sqrt',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       n_estimators=100, n_jobs=-1, oob_score=False,\n                       random_state=8111, verbose=0, warm_start=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='sqrt',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       n_estimators=100, n_jobs=-1, oob_score=False,\n                       random_state=8111, verbose=0, warm_start=False)"
  },
  {
    "objectID": "prediksi_pycaret.html#evaluasi",
    "href": "prediksi_pycaret.html#evaluasi",
    "title": "3  Analisis dan Prediksi pada dataset Heart failure clinical records Menggunakan Pycaret.",
    "section": "3.5 Evaluasi",
    "text": "3.5 Evaluasi\n\n3.5.1 Menyimpan Model terbaik Menggunakan Pycaret\nBisa dilihat diatas bahwa model terbaik salah satunya adalah Random Forest Classifier. maka kita simpan model Random Forest Classifier untuk prediksi nantinya.\n\n3.5.1.1 Random Forest Classifier:\nRandom Forest adalah algoritma machine learning yang termasuk dalam kategori ensemble learning. Ensemble learning menggabungkan prediksi dari beberapa model untuk meningkatkan performa dan ketahanan terhadap overfitting. Random Forest dapat digunakan untuk tugas klasifikasi (seperti prediksi kategori) dan regresi (prediksi nilai numerik).\n\n\n3.5.1.2 Cara Kerja:\n\nPembuatan Banyak Pohon (Trees):\n\nRandom Forest terdiri dari sejumlah besar pohon keputusan yang dibuat secara acak. Setiap pohon dalam Random Forest dibuat berdasarkan subset acak dari data pelatihan dan fitur-fiturnya.\n\nBootstrap Sampling (Bootstrapped Dataset):\n\nPada setiap langkah pembuatan pohon, dilakukan bootstrap sampling, yaitu pengambilan sampel acak dengan penggantian dari dataset pelatihan. Beberapa data dapat muncul lebih dari sekali, dan beberapa mungkin tidak dipilih.\n\nPemilihan Fitur Secara Acak:\n\nPada setiap langkah pembuatan pohon, juga dilakukan pemilihan acak dari fitur-fitur yang tersedia. Ini membantu dalam menciptakan variasi antar pohon.\n\nPembuatan Pohon Keputusan:\n\nSetiap pohon dibuat menggunakan data sampel dari bootstrap dan fitur yang dipilih secara acak. Pemisahan (split) di setiap node pohon dilakukan berdasarkan kriteria seperti Gini Impurity untuk klasifikasi atau Mean Squared Error untuk regresi.\n\nVoting (Klasifikasi):\n\nUntuk tugas klasifikasi, setelah semua pohon selesai membuat prediksi, hasilnya diambil berdasarkan mayoritas voting. Kelas dengan voting terbanyak dianggap sebagai prediksi akhir.\n\n\njika (N) adalah jumlah pohon dalam Random Forest, dan (h_i(x)) adalah hasil prediksi dari pohon ke-i, maka hasil akhir (H(x)) dari Random Forest dapat dihitung sebagai berikut:\nRumus untuk Klasifikasi Random Forest:\n$[ H(x) = (h_1(x), h_2(x), , h_N(x)) $]\npenjelasan:\n\n\\((H(x)\\)) adalah prediksi akhir dari ensemble model.\n\\((h_i(x)\\)) adalah prediksi dari model ke-\\((i\\)).\n\\((\\text{{mode}}\\)) merujuk pada nilai yang paling sering muncul atau kelas yang paling sering diprediksi di antara prediksi model-individu.\n\nRumus ini, sesuai dengan prinsip mayoritas voting pada ensambel model seperti Random Forest. Model ensambel, seperti Random Forest, cenderung memberikan performa yang baik dalam berbagai jenis dataset, termasuk dataset kesehatan seperti “Heart failure clinical records”.\nDalam kasus dataset kesehatan seperti ini, Random Forest bisa menjadi pilihan yang baik karena:\n\nRobust terhadap Overfitting: Random Forest mampu mengatasi masalah overfitting yang mungkin muncul pada pohon keputusan tunggal, karena hasil mayoritas dari banyak pohon keputusan.\nTidak Sensitif terhadap Outliers: Random Forest dapat menangani data yang tidak seimbang dan keberadaan outlier dalam dataset.\nInterpretability: Meskipun Random Forest cenderung tidak seinterpretatif pohon keputusan tunggal, tetapi masih memberikan pemahaman yang baik tentang pentingnya fitur dalam membuat keputusan.\nHandling Fitur Numerik dan Kategorikal: Random Forest dapat menangani baik fitur numerik maupun kategorikal tanpa memerlukan transformasi khusus.\nPerforma yang Baik secara Umum: Random Forest umumnya memberikan performa yang baik tanpa perlu penyesuaian parameter yang terlalu rumit.\n\n\nbest_model = create_model('rf')\n\n# Simpan model terbaik ke dalam file pickle\nmodel_filename = 'best_model.pkl'\nwith open(model_filename, 'wb') as file:\n    pickle.dump(best_model, file)\n\n# Load model dari file pickle\nwith open(model_filename, 'rb') as file:\n    loaded_model = pickle.load(file)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.7619\n0.7806\n0.5714\n0.6667\n0.6154\n0.4444\n0.4472\n\n\n1\n0.8095\n0.9184\n0.4286\n1.0000\n0.6000\n0.5000\n0.5774\n\n\n2\n0.9048\n0.9592\n0.8571\n0.8571\n0.8571\n0.7857\n0.7857\n\n\n3\n0.7619\n0.9286\n1.0000\n0.5833\n0.7368\n0.5455\n0.6124\n\n\n4\n0.7619\n0.8061\n0.4286\n0.7500\n0.5455\n0.4000\n0.4287\n\n\n5\n0.9524\n0.9796\n0.8571\n1.0000\n0.9231\n0.8889\n0.8944\n\n\n6\n0.8571\n0.9235\n0.7143\n0.8333\n0.7692\n0.6667\n0.6708\n\n\n7\n0.8571\n0.9222\n0.8333\n0.7143\n0.7692\n0.6667\n0.6708\n\n\n8\n0.8571\n0.8667\n0.5000\n1.0000\n0.6667\n0.5882\n0.6455\n\n\n9\n0.7000\n0.8452\n0.5000\n0.5000\n0.5000\n0.2857\n0.2857\n\n\nMean\n0.8224\n0.8930\n0.6690\n0.7905\n0.6983\n0.5772\n0.6019\n\n\nStd\n0.0731\n0.0622\n0.1979\n0.1697\n0.1294\n0.1725\n0.1689\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Prediksi Death_Event Menggunakan Random Forest Classifier (rf) dengan Data Baru\nPrediksi Menggunakan Random Forest untuk Klasifikasi:\n\nPersiapkan Data Baru:\n\nMasukkan data baru ke dalam model dengan memberikan nilai untuk setiap fitur yang sesuai.\n\nLakukan Prediksi pada Setiap Pohon:\n\nSetiap pohon dalam Random Forest memberikan prediksi berdasarkan data baru.\n\nAggregasi Hasil Prediksi:\n\nHasil prediksi dari setiap pohon diambil dan dihitung mayoritas voting.\n\nTentukan Kelas Akhir:\n\nHasil akhir diambil berdasarkan mayoritas voting sebagai kelas prediksi akhir.\n\n\nContoh Kode:\n# Membaca data baru yang akan diprediksi\nnew_data = pd.DataFrame([[age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction, high_blood_pressure, platelets, serum_creatinine, serum_sodium, sex, smoking, time]], columns=X.columns)\n\n# Memuat model terbaik dari file pickle\nloaded_model = load_model('best_model')\n\n# Melakukan prediksi pada data baru\npredictions = predict_model(loaded_model, data=new_data)\n\n# Menampilkan hasil prediksi\nif predictions['prediction_label'].iloc[0] == 1:\n    print(\"Hasil Prediksi: Pasien berisiko mengalami DEATH_EVENT\")\nelse:\n    print(\"Hasil Prediksi: Pasien tidak berisiko mengalami DEATH_EVENT\")\nDalam kode ini, prediction_label adalah kolom yang berisi prediksi kelas (0 atau 1) dari model Random Forest untuk tugas klasifikasi. Jika nilai Label adalah 1, itu berarti pasien berisiko mengalami DEATH_EVENT; jika 0, itu berarti pasien tidak berisiko.\n\n# Membaca data baru yang akan diprediksi\nage = float(input(\"Masukkan nilai Age: \"))\nanaemia = int(input(\"Masukkan nilai Anaemia (0 untuk Tidak, 1 untuk Ya): \"))\ncreatinine_phosphokinase = float(input(\"Masukkan nilai Creatinine Phosphokinase: \"))\ndiabetes = int(input(\"Masukkan nilai Diabetes (0 untuk Tidak, 1 untuk Ya): \"))\nejection_fraction = float(input(\"Masukkan nilai Ejection Fraction: \"))\nhigh_blood_pressure = int(input(\"Masukkan nilai High Blood Pressure (0 untuk Tidak, 1 untuk Ya): \"))\nplatelets = float(input(\"Masukkan nilai Platelets: \"))\nserum_creatinine = float(input(\"Masukkan nilai Serum Creatinine: \"))\nserum_sodium = float(input(\"Masukkan nilai Serum Sodium: \"))\nsex = int(input(\"Masukkan nilai Sex (0 untuk Perempuan, 1 untuk Laki-laki): \"))\nsmoking = int(input(\"Masukkan nilai Smoking (0 untuk Tidak, 1 untuk Ya): \"))\ntime = float(input(\"Masukkan nilai Time: \"))\n\n# Membuat data baru untuk prediksi\nnew_data = pd.DataFrame([[age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction, high_blood_pressure, platelets, serum_creatinine, serum_sodium, sex, smoking, time]], columns=X.columns)\n\n# Memuat model terbaik dari file pickle\nloaded_model = load_model('best_model')\n\n# Melakukan prediksi pada data baru\npredictions = predict_model(loaded_model, data=new_data)\n\n# Menampilkan hasil prediksi\nif predictions['prediction_label'].iloc[0] == 1:\n    print(\"Hasil Prediksi: Pasien berisiko mengalami DEATH_EVENT\")\nelse:\n    print(\"Hasil Prediksi: Pasien tidak berisiko mengalami DEATH_EVENT\")\n\nMasukkan nilai Age: 60\nMasukkan nilai Anaemia (0 untuk Tidak, 1 untuk Ya): 0\nMasukkan nilai Creatinine Phosphokinase: 528\nMasukkan nilai Diabetes (0 untuk Tidak, 1 untuk Ya): 0\nMasukkan nilai Ejection Fraction: 20\nMasukkan nilai High Blood Pressure (0 untuk Tidak, 1 untuk Ya): 1\nMasukkan nilai Platelets: 125000\nMasukkan nilai Serum Creatinine: 1.4\nMasukkan nilai Serum Sodium: 13\nMasukkan nilai Sex (0 untuk Perempuan, 1 untuk Laki-laki): 1\nMasukkan nilai Smoking (0 untuk Tidak, 1 untuk Ya): 0\nMasukkan nilai Time: 9\nTransformation Pipeline and Model Successfully Loaded\n\n\n\n\n\nHasil Prediksi: Pasien berisiko mengalami DEATH_EVENT"
  },
  {
    "objectID": "prediksi_pycaret.html#deployment",
    "href": "prediksi_pycaret.html#deployment",
    "title": "3  Analisis dan Prediksi pada dataset Heart failure clinical records Menggunakan Pycaret.",
    "section": "3.6 Deployment",
    "text": "3.6 Deployment\n\n3.6.1 Link Streamlit/Aplikasi Prediksi pada dataset Heart failure clinical records:\nhttps://psd-prediksi-jantung.streamlit.app"
  }
]